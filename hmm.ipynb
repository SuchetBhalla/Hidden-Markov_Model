{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c203ca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6092eb70",
   "metadata": {},
   "source": [
    "**Definition of HMM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff6c00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: D. Jurafsky, J. H. Martin, \"Speech and Language Processing: 3rd Edition. Chapter A: Hidden Markov Models\", 7th Jan 2023\n",
    "# Hyperlink: https://web.stanford.edu/~jurafsky/slp3/\n",
    "\n",
    "class HMM:\n",
    "    \n",
    "    # The arguments are the training-data and the no. of data to use for training\n",
    "    def __init__(self, corpus, m):\n",
    "        \n",
    "        # Pre-process the training-corpus using Stemming & ReGex\n",
    "        self.corpus = self.preprocess(corpus[:m])\n",
    "\n",
    "        # Creates two sets, States & Vocabulary\n",
    "        self.Q, self.V = self.analyze()\n",
    "        \n",
    "        self.N = len( self.Q )\n",
    "        self.D = len( self.V )\n",
    "        \n",
    "        # Create hashtables, to retrieve the index of a token (or tag) in time O(1)\n",
    "        self.create_hashtables()\n",
    "        \n",
    "        # Intializes the model with the training-corpus\n",
    "        self.initialize_model()\n",
    "  \n",
    "    # Trains the model\n",
    "    def fit(self):\n",
    "        \n",
    "        #\n",
    "        self.log_start()\n",
    "        # Stores the log-likehood per epoch; it is calculated during training\n",
    "        self.costs = []\n",
    "        # An Expectation-Maximization algorithm\n",
    "        self.Baum_Welch_algorithm()\n",
    "        #\n",
    "        self.log_end()\n",
    "\n",
    "    \"\"\"\n",
    "    1. Input: A dictionary of untagged-sentences\n",
    "    2. Output: A list of tagged-sentences\n",
    "    Here each sentence is a list of tokens\n",
    "    \"\"\"\n",
    "    def predict(self, test_data):\n",
    "\n",
    "        # Pre-process using Stemming & ReGex\n",
    "        self.test_data = self.preprocess_2(test_data)\n",
    "        \n",
    "        # Generates predictions\n",
    "        ans = []\n",
    "        print(\"Generating predictions:\")\n",
    "        for key, sentence in tqdm(self.test_data):\n",
    "            \n",
    "            self.viterbi_algorithm(sentence)\n",
    "            ans.append([key, list( zip(test_data[key], self.bestpath) )])\n",
    "\n",
    "        return ans\n",
    "    \n",
    "    # Wrapper function to initialize the model\n",
    "    def initialize_model(self):\n",
    "        \n",
    "        #\n",
    "        self.A, self.B, self.Pi = self.initialize()\n",
    "        # Normalizes the vectors A[i, :], B[:, j], Pi[:]\n",
    "        self.rectify()\n",
    "    \n",
    "    #Reads the training-corpus to create two sets, one each for states & vocabulary\n",
    "    def analyze(self):\n",
    "\n",
    "        # Read the possible states and tokens\n",
    "        vocabulary = set()\n",
    "        states = set()\n",
    "\n",
    "        print(\"Initializing Q, V:\")\n",
    "        for sentence in tqdm(self.corpus):\n",
    "            for token, tag in sentence:\n",
    "                states.add(tag)\n",
    "                vocabulary.add(token)\n",
    "\n",
    "        # Any Out-Of-Vocabulary token (in the testing data) shall be replaced by the token\"UNK\"\n",
    "        vocabulary.add(\"UNK\")\n",
    "\n",
    "        v = list(vocabulary)\n",
    "        q = list(states)\n",
    "        v.sort()\n",
    "        q.sort()\n",
    "\n",
    "        return q, v\n",
    "    \n",
    "    \n",
    "    # Creates hashtables to map token & tags to their indicies in the lists self.V & self.Q\n",
    "    def create_hashtables(self):\n",
    "        \n",
    "        self.idx_to_token = dict(enumerate(self.V))\n",
    "        \n",
    "        self.token_to_idx = {}\n",
    "        for idx, tok in self.idx_to_token.items():\n",
    "            self.token_to_idx[tok] = idx\n",
    "        \n",
    "        self.idx_to_tag = dict(enumerate(self.Q))\n",
    "        \n",
    "        self.tag_to_idx = {}\n",
    "        for idx, tag in self.idx_to_tag.items():\n",
    "            self.tag_to_idx[tag] = idx\n",
    "\n",
    "    \"\"\"\n",
    "    The functions 'log_start' & 'log_end shall' print the change in\n",
    "    the model's parameters, i.e., A, B, Pi, before & after training\n",
    "    \"\"\"\n",
    "    def log_start(self):\n",
    "        \n",
    "        self.AA  = np.copy(self.A)\n",
    "        self.BB  = np.copy(self.B)\n",
    "        self.Pii = np.copy(self.Pi)\n",
    "        \n",
    "    def log_end(self):\n",
    "        \n",
    "        delta_A  = np.linalg.norm( self.A - self.AA )   / np.linalg.norm(self.AA)\n",
    "        delta_B  = np.linalg.norm( self.B - self.BB )   / np.linalg.norm(self.BB)\n",
    "        delta_pi = np.linalg.norm( self.Pi - self.Pii ) / np.linalg.norm(self.Pii)\n",
    "        \n",
    "        print(\"\\nAfter training,\")\n",
    "        print(f\"Change in A: {round(100*delta_A)} %\")\n",
    "        print(f\"Change in B: {round(100*delta_B)} %\")\n",
    "        print(f\"Change in Pi: {round(100*delta_pi)} %\")\n",
    "        \n",
    "    # Normalizes the vectors A[i, :], B[:, j], Pi[:]\n",
    "    def rectify(self):\n",
    "\n",
    "        den = np.sum(self.Pi)\n",
    "        if den:\n",
    "            self.Pi /= den\n",
    "\n",
    "        for i in range(self.N):\n",
    "            den = np.sum(self.A[i])\n",
    "            if den:\n",
    "                self.A[i] /= den\n",
    "\n",
    "        for i in range(self.N):\n",
    "            den = np.sum(self.B[:, i])\n",
    "            if den:\n",
    "                self.B[:, i] /= den\n",
    "                \n",
    "    \"\"\"\n",
    "    1. Input 'sent' must be a tagged-sentence, i.e., a list of tuples, of the form (token, tag)\n",
    "    2. Output: A prediction\n",
    "    \"\"\"\n",
    "    def predict_for_sentence(self, sentence):\n",
    "\n",
    "        # Pre-process using Stemming & ReGex\n",
    "        tmp = self.preprocess([sentence])[0]\n",
    "        \n",
    "        # Extracts the tokens from the tagged-sentence\n",
    "        sent = []\n",
    "        for tok, tag in tmp:\n",
    "            sent.append(tok)\n",
    "\n",
    "        # Predicts the tags\n",
    "        self.viterbi_algorithm(sent)\n",
    "\n",
    "        #\n",
    "        print(\"Sentence:\", end= \" \")\n",
    "        for tok, tag in sentence:\n",
    "            print(tok, end= \" \")\n",
    "\n",
    "        print(\"\\nTarget:\", end= \" \")\n",
    "        for tok, tag in sentence:\n",
    "            print(tag, end= \"->\")\n",
    "        print(\"END\")\n",
    "\n",
    "        print(\"\\nPrediction:\", end= \" \")\n",
    "        for tag in self.bestpath:\n",
    "            print(tag, end= \"->\")\n",
    "        print(\"END\")\n",
    "        \n",
    "        val = self.best_score\n",
    "        if val > 0:\n",
    "            val = round( np.log(val), 3)\n",
    "        else:\n",
    "            val = float('inf')\n",
    "        \n",
    "        print(\"\\nLog-Likelihood\", val)\n",
    "        \n",
    "    \"\"\"\n",
    "    Pre-process the training-Corpus (i.e., tagged sentences) using Stemming & ReGex\n",
    "    The argument 'corpus' is a list of lists\n",
    "    \"\"\"\n",
    "    def preprocess(self, corpus):\n",
    "\n",
    "        # modules\n",
    "        import re\n",
    "        from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "        stmr = SnowballStemmer(language= \"english\")\n",
    "        sp = self.regex_strings()\n",
    "\n",
    "        print(\"Preprocessing with ReGex & Stemming:\")\n",
    "        data = []\n",
    "        for sentence in tqdm(corpus):\n",
    "            tmp = []\n",
    "            for token, tag in sentence:\n",
    "                a = re.sub(sp[0], \"(\", token)\n",
    "                b = re.sub(sp[1], \")\", a)\n",
    "                c = re.sub(sp[2], \"-\", b)\n",
    "                d = re.sub(sp[3], \"*\", c)\n",
    "                e = re.sub(sp[4], \"3\", d)\n",
    "                f = re.sub(sp[5], \"3\", e)\n",
    "                g = re.sub(\"-3\", \"3\", f)\n",
    "                h = stmr.stem(g)\n",
    "                tmp.append( (h, tag) )\n",
    "            data.append(tmp)\n",
    "\n",
    "        del re, SnowballStemmer, stmr\n",
    "        return data\n",
    "\n",
    "    \"\"\"\n",
    "    Pre-process the testing-Corpus (i.e., untagged sentences) using Stemming & ReGex\n",
    "    The argument 'test_data' is a dictionary\n",
    "    \"\"\"\n",
    "    def preprocess_2(self, test_data):\n",
    "        \n",
    "        # modules\n",
    "        import re\n",
    "        from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "        stmr = SnowballStemmer(language= \"english\")\n",
    "        sp = self.regex_strings()\n",
    "        \n",
    "        print(\"Preprocessing with ReGex & Stemming:\")\n",
    "        data = []\n",
    "        for key, sentence in tqdm(test_data.items()):\n",
    "            tmp = []\n",
    "            for token in sentence:\n",
    "                a = re.sub(sp[0], \"(\", token)\n",
    "                b = re.sub(sp[1], \")\", a)\n",
    "                c = re.sub(sp[2], \"-\", b)\n",
    "                d = re.sub(sp[3], \"*\", c)\n",
    "                e = re.sub(sp[4], \"3\", d)\n",
    "                f = re.sub(sp[5], \"3\", e)\n",
    "                g = re.sub(\"-3\", \"3\", f)\n",
    "                h = stmr.stem(g)\n",
    "                tmp.append( h )\n",
    "            data.append((key, tmp))\n",
    "\n",
    "        del re, SnowballStemmer, stmr\n",
    "        return data\n",
    "\n",
    "    # This function stores the search-patterns, which shall be used to pre-process the corpora\n",
    "    def regex_strings(self):\n",
    "\n",
    "        # Search patterns\n",
    "        s1 = r\"[\\[({]\"\n",
    "        s2 = r\"[\\])}]\"\n",
    "        s3 = r\"[-]+\"\n",
    "        s4 = r\"[*]+[-]?[a-z]+[-]?[a-z]*\"\n",
    "        s5 = r\"[$]?[.]?[']?[0-9]+[*''/,-.%$:0-9]*\"\n",
    "        s6 = r\"3[\\[][a-z][\\]][\\[]?[3]?[\\]]?\"\n",
    "        #s7 = r\"[!?;]+\"\n",
    "        \n",
    "        return [s1, s2, s3, s4, s5, s6] #, s7\n",
    " \n",
    "    # Initializes the tensors A, B, pi\n",
    "    def initialize(self):\n",
    "\n",
    "        # 'ls' is used for (Laplace-)Smoothing\n",
    "        ls = 0.1\n",
    "        \n",
    "        A  = np.full((self.N, self.N), ls)\n",
    "        B  = np.full((self.D, self.N), ls)\n",
    "        Pi = np.full((self.N), ls)\n",
    "\n",
    "        #\n",
    "        print(\"Initializing A, B, Pi\")\n",
    "        for sentence in tqdm(self.corpus):\n",
    "\n",
    "            # Updates Pi\n",
    "            tag = sentence[0][1]\n",
    "            Pi[self.Q.index(tag)] += 1\n",
    "\n",
    "            # Updates A, B\n",
    "            T_1 = len(sentence) -1\n",
    "            for t in range(T_1):\n",
    "\n",
    "                token, tag = sentence[t]\n",
    "                next_tag = sentence[t+1][1]\n",
    "\n",
    "                i = self.tag_to_idx[tag]\n",
    "                j = self.tag_to_idx[next_tag]\n",
    "                k = self.token_to_idx[token]\n",
    "\n",
    "                A[i, j] += 1\n",
    "                B[k, i] += 1\n",
    "\n",
    "            # Edge Case for matrix B: This is the last tuple in the sentence\n",
    "            token, tag = sentence[-1]\n",
    "            i = self.tag_to_idx[tag]\n",
    "            k = self.token_to_idx[token]\n",
    "            B[k, i] += 1\n",
    "\n",
    "        # Converts the frequencies to probabilities\n",
    "        x = self.N * ls\n",
    "        den = np.sum(Pi) + x\n",
    "        Pi /= den\n",
    "\n",
    "        for i in range(self.N):\n",
    "            den = np.sum(A[i]) + x\n",
    "            A[i] = A[i] / den\n",
    "\n",
    "        for i in range(self.N):\n",
    "            den = np.sum(B[:, i]) + x\n",
    "            B[:, i] /= den\n",
    "\n",
    "        return A, B, Pi\n",
    "\n",
    "    # Calculates the Forward probabilities\n",
    "    def forward_algorithm(self, sentence):\n",
    "\n",
    "        # Creates the alpha-matrix\n",
    "        T = len(sentence)\n",
    "        self.alpha = np.zeros((T, self.N))\n",
    "\n",
    "        # Initialization\n",
    "        v = self.token_to_idx[sentence[0][0]]\n",
    "        np.multiply(self.B[v], self.Pi, out= self.alpha[0])\n",
    "\n",
    "        # Recursion: To build the forward-trellis 'alpha'\n",
    "        for t in range(1, T):\n",
    "            v = self.token_to_idx[sentence[t][0]]\n",
    "            tmp = np.matmul(self.alpha[t-1], self.A)\n",
    "            np.multiply( self.B[v], tmp, out= self.alpha[t] )\n",
    "\n",
    "        # Termination\n",
    "        self.forward_probabilty = np.sum(self.alpha[T-1])\n",
    "\n",
    "\n",
    "    # Calculates the Backward probabilities\n",
    "    def backward_algorithm(self, sentence):\n",
    "\n",
    "        # Creates the beta-matrix\n",
    "        T = len(sentence)\n",
    "        self.beta = np.zeros((T, self.N))\n",
    "\n",
    "        # Initialization\n",
    "        self.beta[T-1] = 1\n",
    "\n",
    "        # Recursion: To build the backward-trellis 'beta'\n",
    "        for t in range(T-2, -1, -1):\n",
    "            t1 = t+1\n",
    "            v = self.token_to_idx[sentence[t1][0]]\n",
    "            tmp = np.multiply(self.B[v], self.beta[t1])\n",
    "            np.matmul(tmp, self.A.T, out= self.beta[t])\n",
    "\n",
    "        # Termination : The following is an unncessary computation\n",
    "        \"\"\"\n",
    "        v = self.token_to_idx(sentence[0][0])\n",
    "        self.backward_probability =  np.mul( self.pi, np.multiply(self.B[v, :], self.beta[0, :]))\n",
    "        \"\"\"\n",
    "\n",
    "    # Calculates the log-likelhood of the input 'sentence' & predicts the state-sequence\n",
    "    def viterbi_algorithm(self, sentence):\n",
    "\n",
    "        # Creates the Viterbi & backtrace matrices\n",
    "        T = len(sentence)\n",
    "        self.viterbi   = np.zeros((T, self.N))\n",
    "        self.backtrace = np.zeros((T, self.N))\n",
    "        \n",
    "        # 1. Initialization\n",
    "        \n",
    "        # If an OOV token occurs (in the testing-corpus), then re-assign it the token 'UNK'\n",
    "        try:\n",
    "            v = self.token_to_idx[sentence[0]]\n",
    "        except KeyError:\n",
    "            v = self.token_to_idx['UNK']\n",
    "        \n",
    "        np.multiply( self.B[v], self.Pi, out= self.viterbi[0] )\n",
    "        self.backtrace[0, :] = np.argmax(self.viterbi[0])\n",
    "\n",
    "        # 2. Recursion: To build the Viterbi-trellis 'viterbi'\n",
    "        for t in range(1, T):\n",
    "            try:\n",
    "                v = self.token_to_idx[sentence[t]]\n",
    "            except KeyError:\n",
    "                v = self.token_to_idx['UNK']\n",
    "                \n",
    "            t_1 = t-1\n",
    "            for j in range(self.N):\n",
    "                tmp = np.multiply(self.viterbi[t_1, :], self.A[:,j])\n",
    "                self.viterbi[t, j]   = np.max(tmp) * self.B[v, j]\n",
    "                self.backtrace[t, j] = np.argmax(tmp)\n",
    "\n",
    "        # 3. Termination\n",
    "        self.best_score = np.max(self.viterbi[T-1])\n",
    "\n",
    "        # Backtracing\n",
    "        indices = [ np.argmax(self.viterbi[T-1]) ]\n",
    "        for t in range(T-2, -1, -1):\n",
    "            indices.append( np.argmax(self.viterbi[t]) )\n",
    "        indices.reverse()\n",
    "        \n",
    "        path = []  \n",
    "        for idx in indices:\n",
    "            path.append(self.Q[idx])\n",
    "        \n",
    "        self.bestpath = path\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs Expectation-Maximization to find [locally-] optimal values for\n",
    "    the Transition & Emission matrices, namely A & B\n",
    "    \"\"\"\n",
    "    def Baum_Welch_algorithm(self):\n",
    "\n",
    "        # 1. Initialize A, B, pi : The following step has already been perform in \"self.fit()\"\n",
    "        #self.A, self.B, self.Pi = self.initialize()\n",
    "\n",
    "        # 2. Iterate until convergence\n",
    "        epochs = 2\n",
    "        itr = 0\n",
    "        n = len(self.corpus)\n",
    "\n",
    "        while itr < epochs:\n",
    "\n",
    "            # Stores the forward-probabilities of a batch\n",
    "            batch = []\n",
    "\n",
    "            # These matrices shall accumulate the A, B, Pi matrices updated by each sentence\n",
    "            a  = np.zeros((self.N, self.N))\n",
    "            b  = np.zeros((self.D, self.N))\n",
    "            pi = np.zeros((self.N))\n",
    "\n",
    "            # Performs full-batch training\n",
    "            for sentence in tqdm(self.corpus):\n",
    "\n",
    "                T = len(sentence)\n",
    "                \n",
    "                # These matrices are temporarily required\n",
    "                xi     = np.zeros((self.N, self.N))\n",
    "                xi_num = np.zeros((T-1, self.N, self.N))\n",
    "                gamma_num  = np.zeros((T, self.N))\n",
    "\n",
    "                # E-step\n",
    "                self.forward_algorithm(sentence)\n",
    "                self.backward_algorithm(sentence)\n",
    "\n",
    "                val = self.forward_probabilty\n",
    "                if val > 0:\n",
    "                    batch.append( np.log(val) )\n",
    "                else:\n",
    "                    # INT_MIN = 10**-323\n",
    "                    batch.append( -323 )\n",
    "\n",
    "                # M-step\n",
    "                \n",
    "                # Calculates Gamma's numerator\n",
    "                np.multiply(self.alpha, self.beta, out= gamma_num)\n",
    "\n",
    "                # Calculates 'b'\n",
    "                for i, v in enumerate(self.V):\n",
    "                    indices = []\n",
    "                    for t, tup in enumerate(sentence):\n",
    "                        if tup[0] == v:\n",
    "                            indices.append(t)\n",
    "                    \n",
    "                    num = np.sum(gamma_num[indices, :], axis=0)\n",
    "                    den = np.sum(gamma_num[:, :], axis=0)\n",
    "                    \n",
    "                    den[ den == 0 ] = 1\n",
    "                    b[i, :] += num / den\n",
    "\n",
    "                # Calculates Pi\n",
    "                if val > 0:\n",
    "                    pi += gamma_num[0, :] / val\n",
    "\n",
    "                # Calculates Xi's numerator\n",
    "                for t in range(T-1):\n",
    "                    t1 = t + 1\n",
    "                    v = self.V.index(sentence[t1][0])\n",
    "                    tmp = np.multiply(self.A, self.B[v, :])\n",
    "                    np.multiply(tmp, self.beta[t1, :], out= xi_num[t])\n",
    "                \n",
    "                np.multiply(xi_num, self.alpha[:-1, :, None], out= xi_num)\n",
    "                \n",
    "                # Calculates 'a'\n",
    "                xi = np.sum(xi_num, axis= 0)           \n",
    "                den = np.sum( xi, axis= 1)\n",
    "                den[ den == 0 ] = 1\n",
    "                a += xi/den[:, None]\n",
    "\n",
    "            # Division by n gives equal weight to each sentence\n",
    "            self.A = a/n\n",
    "            self.B = b/n\n",
    "            self.Pi = pi/n\n",
    "\n",
    "            # Ensures that each row of A, columns of B, & Pi add upto 1\n",
    "            self.rectify()\n",
    "\n",
    "            # Store the collective log-likelihood of the batch\n",
    "            self.costs.append( sum(batch)/n )\n",
    "\n",
    "            #\n",
    "            print(f\"\\nEpoch {itr+1}:\", round(self.costs[itr], 3))\n",
    "            itr += 1\n",
    "\n",
    "        print(\"HMM: Training is Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7439eefb",
   "metadata": {},
   "source": [
    "**Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3705de63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the training-Corpus:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47340it [00:14, 3198.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loads the training data\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "data = []\n",
    "print(\"Reading the training-Corpus:\")\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    data.append(ast.literal_eval(row['tagged_sentence']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c130f0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing with ReGex & Stemming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 47/47 [00:00<00:00, 986.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Q, V:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing A, B, Pi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 47/47 [00:00<00:00, 2989.89it/s]\n"
     ]
    }
   ],
   "source": [
    "#del obj\n",
    "n = len(data) // 1000\n",
    "obj = HMM(data, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6fd7ef",
   "metadata": {},
   "source": [
    "**Before training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70cfd57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing with ReGex & Stemming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Merger proposed \n",
      "Target: NN->VB->END\n",
      "\n",
      "Prediction: NN->VB->END\n",
      "\n",
      "Log-Likelihood -14.271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "obj.predict_for_sentence(data[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2386b062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 47/47 [00:00<00:00, 47.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: -114.868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 47/47 [00:00<00:00, 57.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: -110.103\n",
      "HMM: Training is Complete.\n",
      "\n",
      "After training,\n",
      "Change in A: 35 %\n",
      "Change in B: 45 %\n",
      "Change in Pi: 40 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "obj.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e810c5dd",
   "metadata": {},
   "source": [
    "**After training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a02935f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing with ReGex & Stemming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Merger proposed \n",
      "Target: NN->VB->END\n",
      "\n",
      "Prediction: AT->JJ->END\n",
      "\n",
      "Log-Likelihood -10.311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "obj.predict_for_sentence(data[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "539bfd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the testing-Corpus:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [00:00, 5081.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loads the testing-data\n",
    "df2 = pd.read_csv('./data/test.csv')\n",
    "test_data = {}\n",
    "print(\"Reading the testing-Corpus:\")\n",
    "for index, row in tqdm(df2.iterrows()):\n",
    "    test_data[row['id']] = ast.literal_eval(row['untagged_sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5fb121",
   "metadata": {},
   "source": [
    "**Generates Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03a7eef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing with ReGex & Stemming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:02<00:00, 1514.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:43<00:00, 91.11it/s]\n"
     ]
    }
   ],
   "source": [
    "ans = obj.predict(test_data)\n",
    "pred = pd.DataFrame(data= ans, columns =['id', 'tagged_sentence'])\n",
    "pred.to_csv('./data/pred.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a85cd1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
